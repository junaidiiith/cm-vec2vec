{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c430f732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gan_pytorch.py\n",
    "# A complete DCGAN training script from scratch (PyTorch).\n",
    "# - Trains on MNIST (default) or CIFAR-10\n",
    "# - Saves sample grids per epoch to ./samples\n",
    "# - Saves checkpoints to ./checkpoints\n",
    "#\n",
    "# Usage:\n",
    "#   python gan_pytorch.py --dataset mnist --epochs 20\n",
    "#   python gan_pytorch.py --dataset cifar10 --epochs 50 --batch_size 128 --lr 2e-4\n",
    "# Generate images from a trained checkpoint:\n",
    "#   python gan_pytorch.py --generate 64 --checkpoint ./checkpoints/G_epoch_20.pt --dataset mnist\n",
    "#\n",
    "# Tip: For CPU only, it'll still run (just slower).\n",
    "\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "from typing import Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, utils as vutils\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import glob\n",
    "\n",
    "import torchvision.transforms as T\n",
    "\n",
    "\n",
    "\n",
    "def seed_all(seed=42):\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "def make_dirs():\n",
    "    Path(\"samples_wgan\").mkdir(exist_ok=True)\n",
    "    Path(\"checkpoints_wgan\").mkdir(exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "def weights_init(m):\n",
    "    \"\"\"Initialize weights the DCGAN way (from the original paper).\"\"\"\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Conv\") != -1 or classname.find(\"ConvTranspose\") != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find(\"BatchNorm\") != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "def cosine_sim(a: torch.Tensor, b: torch.Tensor, eps: float = 1e-8) -> torch.Tensor:\n",
    "    a_n = a / (a.norm(dim=-1, keepdim=True) + eps)\n",
    "    b_n = b / (b.norm(dim=-1, keepdim=True) + eps)\n",
    "    return (a_n * b_n).sum(dim=-1)\n",
    "\n",
    "def pairwise_dot(x: torch.Tensor) -> torch.Tensor:\n",
    "    return x @ x.t()\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Data\n",
    "# -----------------------------\n",
    "def get_data(\n",
    "    dataset: str,\n",
    "    image_size: int,\n",
    "    batch_size: int,\n",
    "    num_workers: int = 4,\n",
    ") -> Tuple[torch.utils.data.DataLoader, int]:\n",
    "    dataset = dataset.lower()\n",
    "    if dataset not in {\"mnist\", \"fashion-mnist\", \"cifar10\"}:\n",
    "        raise ValueError(\"dataset must be one of: mnist, fashion-mnist, cifar10\")\n",
    "\n",
    "    if dataset in {\"mnist\", \"fashion-mnist\"}:\n",
    "        nc = 1\n",
    "        mean = (0.5,)\n",
    "        std = (0.5,)\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize(image_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean, std),\n",
    "        ])\n",
    "        if dataset == \"mnist\":\n",
    "            ds = datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
    "        else:\n",
    "            ds = datasets.FashionMNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
    "    else:\n",
    "        nc = 3\n",
    "        mean = (0.5, 0.5, 0.5)\n",
    "        std = (0.5, 0.5, 0.5)\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize(image_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean, std),\n",
    "        ])\n",
    "        ds = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=transform)\n",
    "\n",
    "    loader = torch.utils.data.DataLoader(\n",
    "        ds, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True\n",
    "    )\n",
    "    return loader, nc\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1d5682",
   "metadata": {},
   "source": [
    "# DCGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4436c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Models: DCGAN-style Generator / Discriminator\n",
    "# -----------------------------\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, nz: int, ngf: int, nc: int):\n",
    "        \"\"\"\n",
    "        nz  = latent dim\n",
    "        ngf = generator feature multiplier\n",
    "        nc  = number of channels (1 for MNIST, 3 for CIFAR-10)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            # input Z: (N, nz, 1, 1) -> (N, ngf*8, 4, 4)\n",
    "            nn.ConvTranspose2d(nz, ngf * 8, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 8),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            # (N, ngf*8, 4, 4) -> (N, ngf*4, 8, 8)\n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            # (N, ngf*4, 8, 8) -> (N, ngf*2, 16, 16)\n",
    "            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            # (N, ngf*2, 16, 16) -> (N, ngf, 32, 32)\n",
    "            nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            # (N, ngf, 32, 32) -> (N, nc, 64, 64)\n",
    "            nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False),\n",
    "            nn.Tanh(),  # outputs in [-1, 1]\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.main(z)\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, nc: int, ndf: int):\n",
    "        \"\"\"\n",
    "        nc  = number of channels\n",
    "        ndf = discriminator feature multiplier\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            # (N, nc, 64, 64) -> (N, ndf, 32, 32)\n",
    "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            # -> (N, ndf*2, 16, 16)\n",
    "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            # -> (N, ndf*4, 8, 8)\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            # -> (N, ndf*8, 4, 4)\n",
    "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            # -> (N, 1, 1, 1)\n",
    "            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n",
    "            # No Sigmoid: we'll use BCEWithLogitsLoss for stability\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.main(x)\n",
    "        return out.view(-1)  # logits shape: (N,)\n",
    "\n",
    "# -----------------------------\n",
    "# Training\n",
    "# -----------------------------\n",
    "def gan_train(args):\n",
    "    make_dirs()\n",
    "    seed_all(args.seed)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.cpu else \"cpu\")\n",
    "    print(f\"Device: {device}\")\n",
    "\n",
    "    # Data\n",
    "    dataloader, nc = get_data(args.dataset, args.image_size, args.batch_size, args.num_workers)\n",
    "    print(f\"Dataset: {args.dataset} | nc={nc} | batches/epoch={len(dataloader)}\")\n",
    "\n",
    "    # Models\n",
    "    netG = Generator(args.nz, args.ngf, nc).to(device)\n",
    "    netD = Discriminator(nc, args.ndf).to(device)\n",
    "    netG.apply(weights_init)\n",
    "    netD.apply(weights_init)\n",
    "\n",
    "    # Loss and Optim\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizerD = optim.Adam(netD.parameters(), lr=args.lr, betas=(args.beta1, 0.999))\n",
    "    optimizerG = optim.Adam(netG.parameters(), lr=args.lr, betas=(args.beta1, 0.999))\n",
    "\n",
    "    fixed_noise = torch.randn(64, args.nz, 1, 1, device=device)  # for consistent eval\n",
    "\n",
    "    step = 0\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        for i, (real, _) in enumerate(dataloader):\n",
    "            netD.train()\n",
    "            netG.train()\n",
    "            real = real.to(device)\n",
    "\n",
    "            # -------------------------\n",
    "            # Update D: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "            # -------------------------\n",
    "            optimizerD.zero_grad(set_to_none=True)\n",
    "            bsz = real.size(0)\n",
    "\n",
    "            # Real\n",
    "            logits_real = netD(real)\n",
    "            # label smoothing on real can help a tiny bit (0.9 instead of 1.0)\n",
    "            real_labels = torch.full((bsz,), 0.9, device=device)\n",
    "            d_loss_real = criterion(logits_real, real_labels)\n",
    "\n",
    "            # Fake\n",
    "            noise = torch.randn(bsz, args.nz, 1, 1, device=device)\n",
    "            fake = netG(noise).detach()  # detach: do not backprop through G\n",
    "            logits_fake = netD(fake)\n",
    "            fake_labels = torch.zeros(bsz, device=device)\n",
    "            d_loss_fake = criterion(logits_fake, fake_labels)\n",
    "\n",
    "            d_loss = d_loss_real + d_loss_fake\n",
    "            d_loss.backward()\n",
    "            optimizerD.step()\n",
    "\n",
    "            # -------------------------\n",
    "            # Update G: maximize log(D(G(z)))  <=> minimize BCEWithLogitsLoss(logits_fake, 1)\n",
    "            # -------------------------\n",
    "            optimizerG.zero_grad(set_to_none=True)\n",
    "            noise = torch.randn(bsz, args.nz, 1, 1, device=device)\n",
    "            gen = netG(noise)\n",
    "            logits_gen = netD(gen)\n",
    "            g_labels = torch.ones(bsz, device=device)\n",
    "            g_loss = criterion(logits_gen, g_labels)\n",
    "            g_loss.backward()\n",
    "            optimizerG.step()\n",
    "\n",
    "            # Logging\n",
    "            if (i + 1) % args.log_interval == 0:\n",
    "                print(\n",
    "                    f\"Epoch [{epoch}/{args.epochs}] \"\n",
    "                    f\"Step [{i+1}/{len(dataloader)}] \"\n",
    "                    f\"D_loss: {d_loss.item():.4f} | G_loss: {g_loss.item():.4f}\"\n",
    "                )\n",
    "\n",
    "            # Save quick sample grids every sample_interval steps\n",
    "            if step % args.sample_interval == 0:\n",
    "                with torch.no_grad():\n",
    "                    fake_fixed = netG(fixed_noise).cpu()\n",
    "                grid = vutils.make_grid(fake_fixed, nrow=8, normalize=True, value_range=(-1, 1))\n",
    "                vutils.save_image(grid, f\"./samples/epoch{epoch:03d}_step{step:06d}.png\")\n",
    "            step += 1\n",
    "\n",
    "        # End of epoch: save checkpoint + a clean epoch sample\n",
    "        torch.save(netG.state_dict(), f\"./checkpoints/G_epoch_{epoch}.pt\")\n",
    "        torch.save(netD.state_dict(), f\"./checkpoints/D_epoch_{epoch}.pt\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            fake_fixed = netG(fixed_noise).cpu()\n",
    "        grid = vutils.make_grid(fake_fixed, nrow=8, normalize=True, value_range=(-1, 1))\n",
    "        vutils.save_image(grid, f\"./samples/epoch{epoch:03d}.png\")\n",
    "        print(f\"[Epoch {epoch}] checkpoints saved and sample grid written.\")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Image Generation (inference)\n",
    "# -----------------------------\n",
    "def gan_generate(args):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.cpu else \"cpu\")\n",
    "    dataloader, nc = get_data(args.dataset, args.image_size, 1, num_workers=0)  # to pick nc\n",
    "    netG = Generator(args.nz, args.ngf, nc).to(device)\n",
    "    if not args.checkpoint or not os.path.exists(args.checkpoint):\n",
    "        raise FileNotFoundError(\"--checkpoint path is required for --generate mode.\")\n",
    "\n",
    "    netG.load_state_dict(torch.load(args.checkpoint, map_location=device))\n",
    "    netG.eval()\n",
    "\n",
    "    n = args.generate\n",
    "    steps = math.ceil(n / 64)\n",
    "    outdir = Path(\"generated\")\n",
    "    outdir.mkdir(exist_ok=True)\n",
    "    all_imgs = []\n",
    "    with torch.no_grad():\n",
    "        for s in range(steps):\n",
    "            b = min(64, n - s * 64)\n",
    "            z = torch.randn(b, args.nz, 1, 1, device=device)\n",
    "            fake = netG(z).cpu()\n",
    "            all_imgs.append(fake)\n",
    "    imgs = torch.cat(all_imgs, dim=0)\n",
    "    grid = vutils.make_grid(imgs, nrow=8, normalize=True, value_range=(-1, 1))\n",
    "    out_path = outdir / \"gen_grid.png\"\n",
    "    vutils.save_image(grid, out_path)\n",
    "    print(f\"Generated {n} images -> {out_path}\")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Main / CLI\n",
    "# -----------------------------\n",
    "def parse_gan_args():\n",
    "    p = argparse.ArgumentParser(description=\"Train a DCGAN from scratch in PyTorch.\")\n",
    "    p.add_argument(\"--dataset\", type=str, default=\"mnist\", choices=[\"mnist\", \"fashion-mnist\", \"cifar10\"],\n",
    "                   help=\"which dataset to use\")\n",
    "    p.add_argument(\"--image_size\", type=int, default=64, help=\"input/output image size (DCGAN uses 64)\")\n",
    "    p.add_argument(\"--epochs\", type=int, default=20)\n",
    "    p.add_argument(\"--batch_size\", type=int, default=128)\n",
    "    p.add_argument(\"--lr\", type=float, default=2e-4)\n",
    "    p.add_argument(\"--beta1\", type=float, default=0.5, help=\"Adam beta1\")\n",
    "    p.add_argument(\"--nz\", type=int, default=128, help=\"latent dim\")\n",
    "    p.add_argument(\"--ngf\", type=int, default=64, help=\"generator feature maps\")\n",
    "    p.add_argument(\"--ndf\", type=int, default=64, help=\"discriminator feature maps\")\n",
    "    p.add_argument(\"--log_interval\", type=int, default=100)\n",
    "    p.add_argument(\"--sample_interval\", type=int, default=300, help=\"steps between quick sample grids\")\n",
    "    p.add_argument(\"--num_workers\", type=int, default=4)\n",
    "    p.add_argument(\"--seed\", type=int, default=42)\n",
    "    p.add_argument(\"--cpu\", action=\"store_true\", help=\"force CPU even if CUDA is available\")\n",
    "\n",
    "    # generation mode\n",
    "    p.add_argument(\"--generate\", type=int, default=0, help=\">0 to generate N images instead of training\")\n",
    "    p.add_argument(\"--checkpoint\", type=str, default=\"\", help=\"path to G checkpoint for --generate\")\n",
    "    return p.parse_args()\n",
    "\n",
    "\n",
    "\n",
    "# args = parse_args()\n",
    "# if args.generate and args.generate > 0:\n",
    "#     generate(args)\n",
    "# else:\n",
    "#     train(args)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f4d461",
   "metadata": {},
   "source": [
    "# CycleGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72864e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cyclegan_pytorch.py\n",
    "# Minimal but complete CycleGAN training on two unpaired folders (domain A, domain B).\n",
    "# Uses ResNet generators + 70x70 PatchGAN discriminators, LSGAN loss, cycle + identity losses.\n",
    "#\n",
    "# Folder structure (examples):\n",
    "#   --data_a/\n",
    "#       class_dummy/  # ImageFolder needs one level; name doesn't matter\n",
    "#           a1.jpg, a2.png, ...\n",
    "#   --data_b/\n",
    "#       class_dummy/\n",
    "#           b1.jpg, b2.png, ...\n",
    "#\n",
    "# Usage:\n",
    "#   python cyclegan_pytorch.py --data_a ./horses --data_b ./zebras --epochs 50 --img_size 256\n",
    "#   (Put images under data_a/class_dummy/* and data_b/class_dummy/*)\n",
    "#\n",
    "# Outputs:\n",
    "#   ./samples_cyclegan/   -> periodic translated samples\n",
    "#   ./checkpoints_cyclegan/ -> G_AB/G_BA/D_A/D_B checkpoints per epoch\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Data\n",
    "# ---------------------------\n",
    "class UnpairedImageFolder(Dataset):\n",
    "    def __init__(self, root_a: str, root_b: str, img_size: int):\n",
    "        self.files_a = sorted([p for p in glob.glob(os.path.join(root_a, \"**\", \"*.*\"), recursive=True)\n",
    "                               if p.lower().endswith((\".jpg\", \".jpeg\", \".png\"))])\n",
    "        self.files_b = sorted([p for p in glob.glob(os.path.join(root_b, \"**\", \"*.*\"), recursive=True)\n",
    "                               if p.lower().endswith((\".jpg\", \".jpeg\", \".png\"))])\n",
    "        if len(self.files_a) == 0 or len(self.files_b) == 0:\n",
    "            raise RuntimeError(\"Both domain folders must contain images.\")\n",
    "        self.len_a = len(self.files_a)\n",
    "        self.len_b = len(self.files_b)\n",
    "\n",
    "        self.tf = T.Compose([\n",
    "            T.Resize((img_size, img_size), interpolation=T.InterpolationMode.BICUBIC),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        # define epoch size as max of both for better mixing\n",
    "        return max(self.len_a, self.len_b)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        a_path = self.files_a[idx % self.len_a]\n",
    "        b_path = self.files_b[random.randint(0, self.len_b - 1)]\n",
    "        a = self.tf(Image.open(a_path).convert(\"RGB\"))\n",
    "        b = self.tf(Image.open(b_path).convert(\"RGB\"))\n",
    "        return a, b\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Models (CycleGAN)\n",
    "# ---------------------------\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(dim, dim, kernel_size=3, stride=1, padding=0, bias=False),\n",
    "            nn.InstanceNorm2d(dim, affine=False, track_running_stats=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(dim, dim, kernel_size=3, stride=1, padding=0, bias=False),\n",
    "            nn.InstanceNorm2d(dim, affine=False, track_running_stats=False),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)\n",
    "\n",
    "\n",
    "class GeneratorResNet(nn.Module):\n",
    "    # c7s1-64, d128, d256, R* (6 for 128px, 9 for 256px), u128, u64, c7s1-3\n",
    "    def __init__(self, in_ch=3, out_ch=3, n_res=9, ngf=64):\n",
    "        super().__init__()\n",
    "        layers = [\n",
    "            nn.ReflectionPad2d(3),\n",
    "            nn.Conv2d(in_ch, ngf, kernel_size=7, bias=False),\n",
    "            nn.InstanceNorm2d(ngf, affine=False, track_running_stats=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "        ]\n",
    "        # downsample\n",
    "        dim = ngf\n",
    "        for _ in range(2):\n",
    "            layers += [\n",
    "                nn.Conv2d(dim, dim * 2, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "                nn.InstanceNorm2d(dim * 2, affine=False, track_running_stats=False),\n",
    "                nn.ReLU(inplace=True),\n",
    "            ]\n",
    "            dim *= 2\n",
    "        # residuals\n",
    "        for _ in range(n_res):\n",
    "            layers += [ResidualBlock(dim)]\n",
    "        # upsample\n",
    "        for _ in range(2):\n",
    "            layers += [\n",
    "                nn.ConvTranspose2d(dim, dim // 2, kernel_size=3, stride=2, padding=1, output_padding=1, bias=False),\n",
    "                nn.InstanceNorm2d(dim // 2, affine=False, track_running_stats=False),\n",
    "                nn.ReLU(inplace=True),\n",
    "            ]\n",
    "            dim //= 2\n",
    "        # output\n",
    "        layers += [\n",
    "            nn.ReflectionPad2d(3),\n",
    "            nn.Conv2d(dim, out_ch, kernel_size=7),\n",
    "            nn.Tanh(),\n",
    "        ]\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "class PatchDiscriminator(nn.Module):\n",
    "    # 70x70 PatchGAN (no BN in first layer; InstanceNorm afterwards)\n",
    "    def __init__(self, in_ch=3, ndf=64):\n",
    "        super().__init__()\n",
    "        def block(in_c, out_c, norm=True):\n",
    "            layers = [nn.Conv2d(in_c, out_c, 4, 2, 1)]\n",
    "            if norm:\n",
    "                layers += [nn.InstanceNorm2d(out_c, affine=False, track_running_stats=False)]\n",
    "            layers += [nn.LeakyReLU(0.2, inplace=True)]\n",
    "            return layers\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *block(in_ch, ndf, norm=False),\n",
    "            *block(ndf, ndf * 2),\n",
    "            *block(ndf * 2, ndf * 4),\n",
    "            # keep stride 1 in final conv(s) for patch output\n",
    "            nn.Conv2d(ndf * 4, ndf * 8, 4, 1, 1),\n",
    "            nn.InstanceNorm2d(ndf * 8, affine=False, track_running_stats=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(ndf * 8, 1, 4, 1, 1),  # patch logits\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "def init_weights(m):\n",
    "    if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)):\n",
    "        nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "    if isinstance(m, (nn.InstanceNorm2d,)):\n",
    "        if m.weight is not None:\n",
    "            nn.init.ones_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Training\n",
    "# ---------------------------\n",
    "def train_cyclegan(args):\n",
    "    make_dirs()\n",
    "    seed_all(args.seed)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.cpu else \"cpu\")\n",
    "    print(\"Device:\", device)\n",
    "\n",
    "    ds = UnpairedImageFolder(args.data_a, args.data_b, args.img_size)\n",
    "    loader = DataLoader(ds, batch_size=args.batch_size, shuffle=True, num_workers=args.num_workers, pin_memory=True)\n",
    "\n",
    "    # models\n",
    "    n_res = 9 if args.img_size >= 256 else 6\n",
    "    G_AB = GeneratorResNet(3, 3, n_res=n_res, ngf=args.ngf).to(device)\n",
    "    G_BA = GeneratorResNet(3, 3, n_res=n_res, ngf=args.ngf).to(device)\n",
    "    D_A = PatchDiscriminator(3, ndf=args.ndf).to(device)  # distinguishes real A vs fake A (from B->A)\n",
    "    D_B = PatchDiscriminator(3, ndf=args.ndf).to(device)\n",
    "\n",
    "    G_AB.apply(init_weights)\n",
    "    G_BA.apply(init_weights)\n",
    "    D_A.apply(init_weights)\n",
    "    D_B.apply(init_weights)\n",
    "\n",
    "    # losses\n",
    "    adv_criterion = nn.MSELoss()      # LSGAN\n",
    "    cycle_criterion = nn.L1Loss()\n",
    "    id_criterion = nn.L1Loss()\n",
    "\n",
    "    # opt\n",
    "    opt_G = optim.Adam(list(G_AB.parameters()) + list(G_BA.parameters()), lr=args.lr, betas=(0.5, 0.999))\n",
    "    opt_D_A = optim.Adam(D_A.parameters(), lr=args.lr, betas=(0.5, 0.999))\n",
    "    opt_D_B = optim.Adam(D_B.parameters(), lr=args.lr, betas=(0.5, 0.999))\n",
    "\n",
    "    # targets for PatchGAN\n",
    "    def real_like(x): return torch.ones_like(x, device=device)\n",
    "    def fake_like(x): return torch.zeros_like(x, device=device)\n",
    "\n",
    "    step = 0\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        for i, (real_a, real_b) in enumerate(loader):\n",
    "            real_a = real_a.to(device)\n",
    "            real_b = real_b.to(device)\n",
    "\n",
    "            # -----------------------\n",
    "            #  Train Generators G_AB & G_BA\n",
    "            # -----------------------\n",
    "            opt_G.zero_grad(set_to_none=True)\n",
    "\n",
    "            fake_b = G_AB(real_a)\n",
    "            pred_fake_b = D_B(fake_b)\n",
    "            loss_G_AB_adv = adv_criterion(pred_fake_b, real_like(pred_fake_b))\n",
    "\n",
    "            fake_a = G_BA(real_b)\n",
    "            pred_fake_a = D_A(fake_a)\n",
    "            loss_G_BA_adv = adv_criterion(pred_fake_a, real_like(pred_fake_a))\n",
    "\n",
    "            # Cycle\n",
    "            rec_a = G_BA(fake_b)\n",
    "            rec_b = G_AB(fake_a)\n",
    "            loss_cycle = cycle_criterion(rec_a, real_a) + cycle_criterion(rec_b, real_b)\n",
    "            loss_cycle = args.lambda_cyc * loss_cycle\n",
    "\n",
    "            # Identity (optional but stabilizing)\n",
    "            idt_a = G_BA(real_a)\n",
    "            idt_b = G_AB(real_b)\n",
    "            loss_id = id_criterion(idt_a, real_a) + id_criterion(idt_b, real_b)\n",
    "            loss_id = args.lambda_id * loss_id\n",
    "\n",
    "            loss_G = loss_G_AB_adv + loss_G_BA_adv + loss_cycle + loss_id\n",
    "            loss_G.backward()\n",
    "            opt_G.step()\n",
    "\n",
    "            # -----------------------\n",
    "            #  Train D_A\n",
    "            # -----------------------\n",
    "            opt_D_A.zero_grad(set_to_none=True)\n",
    "            pred_real_a = D_A(real_a)\n",
    "            loss_D_A_real = adv_criterion(pred_real_a, real_like(pred_real_a))\n",
    "\n",
    "            with torch.no_grad():\n",
    "                fake_a_det = fake_a\n",
    "            pred_fake_a = D_A(fake_a_det)\n",
    "            loss_D_A_fake = adv_criterion(pred_fake_a, fake_like(pred_fake_a))\n",
    "\n",
    "            loss_D_A = 0.5 * (loss_D_A_real + loss_D_A_fake)\n",
    "            loss_D_A.backward()\n",
    "            opt_D_A.step()\n",
    "\n",
    "            # -----------------------\n",
    "            #  Train D_B\n",
    "            # -----------------------\n",
    "            opt_D_B.zero_grad(set_to_none=True)\n",
    "            pred_real_b = D_B(real_b)\n",
    "            loss_D_B_real = adv_criterion(pred_real_b, real_like(pred_real_b))\n",
    "\n",
    "            with torch.no_grad():\n",
    "                fake_b_det = fake_b\n",
    "            pred_fake_b = D_B(fake_b_det)\n",
    "            loss_D_B_fake = adv_criterion(pred_fake_b, fake_like(pred_fake_b))\n",
    "\n",
    "            loss_D_B = 0.5 * (loss_D_B_real + loss_D_B_fake)\n",
    "            loss_D_B.backward()\n",
    "            opt_D_B.step()\n",
    "\n",
    "            if (i + 1) % args.log_interval == 0:\n",
    "                print(\n",
    "                    f\"[E{epoch}/{args.epochs}] [B{i+1}/{len(loader)}] \"\n",
    "                    f\"G: {loss_G.item():.3f} (adv: {(loss_G_AB_adv+loss_G_BA_adv).item():.3f}, \"\n",
    "                    f\"cyc: {loss_cycle.item():.3f}, id: {loss_id.item():.3f}) | \"\n",
    "                    f\"D_A: {loss_D_A.item():.3f} D_B: {loss_D_B.item():.3f}\"\n",
    "                )\n",
    "\n",
    "            if step % args.sample_interval == 0:\n",
    "                with torch.no_grad():\n",
    "                    a2b = G_AB(real_a[:4])\n",
    "                    b2a = G_BA(real_b[:4])\n",
    "                grid = vutils.make_grid(\n",
    "                    torch.cat([real_a[:4], a2b, real_b[:4], b2a], dim=0),\n",
    "                    nrow=4, normalize=True, value_range=(-1, 1)\n",
    "                )\n",
    "                vutils.save_image(grid, f\"samples_cyclegan/e{epoch:03d}_s{step:06d}.png\")\n",
    "            step += 1\n",
    "\n",
    "        # save per epoch\n",
    "        torch.save(G_AB.state_dict(), f\"checkpoints_cyclegan/G_AB_e{epoch}.pt\")\n",
    "        torch.save(G_BA.state_dict(), f\"checkpoints_cyclegan/G_BA_e{epoch}.pt\")\n",
    "        torch.save(D_A.state_dict(),  f\"checkpoints_cyclegan/D_A_e{epoch}.pt\")\n",
    "        torch.save(D_B.state_dict(),  f\"checkpoints_cyclegan/D_B_e{epoch}.pt\")\n",
    "        print(f\"[Epoch {epoch}] checkpoints saved.\")\n",
    "\n",
    "\n",
    "def parse_cyclegan_args():\n",
    "    p = argparse.ArgumentParser(description=\"CycleGAN from scratch (PyTorch).\")\n",
    "    p.add_argument(\"--data_a\", type=str, required=True, help=\"path to domain A images (root)\")\n",
    "    p.add_argument(\"--data_b\", type=str, required=True, help=\"path to domain B images (root)\")\n",
    "    p.add_argument(\"--img_size\", type=int, default=256)\n",
    "    p.add_argument(\"--batch_size\", type=int, default=2)\n",
    "    p.add_argument(\"--epochs\", type=int, default=50)\n",
    "    p.add_argument(\"--lr\", type=float, default=2e-4)\n",
    "    p.add_argument(\"--lambda_cyc\", type=float, default=10.0)\n",
    "    p.add_argument(\"--lambda_id\", type=float, default=5.0)\n",
    "    p.add_argument(\"--ngf\", type=int, default=64)\n",
    "    p.add_argument(\"--ndf\", type=int, default=64)\n",
    "    p.add_argument(\"--num_workers\", type=int, default=4)\n",
    "    p.add_argument(\"--log_interval\", type=int, default=100)\n",
    "    p.add_argument(\"--sample_interval\", type=int, default=300)\n",
    "    p.add_argument(\"--seed\", type=int, default=42)\n",
    "    p.add_argument(\"--cpu\", action=\"store_true\")\n",
    "    return p.parse_args()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51d30bc",
   "metadata": {},
   "source": [
    "# WGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807c42f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wgan_gp_pytorch.py\n",
    "# WGAN-GP on MNIST/FashionMNIST/CIFAR10. DCGAN-ish generator, critic without BN, gradient penalty.\n",
    "#\n",
    "# Usage:\n",
    "#   python wgan_gp_pytorch.py --dataset cifar10 --epochs 50 --batch_size 128\n",
    "#\n",
    "# Outputs:\n",
    "#   ./samples_wgan/ (images)\n",
    "#   ./checkpoints_wgan/ (G/C state dicts)\n",
    "\n",
    "\n",
    "class Gen(nn.Module):\n",
    "    def __init__(self, nz, ngf, nc):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.ConvTranspose2d(nz, ngf*8, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(ngf*8),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ConvTranspose2d(ngf*8, ngf*4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf*4),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ConvTranspose2d(ngf*4, ngf*2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf*2),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ConvTranspose2d(ngf*2, ngf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, z): return self.net(z)\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    # No BatchNorm in critic for WGAN-GP\n",
    "    def __init__(self, nc, ndf):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(ndf, ndf*2, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(ndf*2, ndf*4, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(ndf*4, ndf*8, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(ndf*8, 1, 4, 1, 0, bias=False),\n",
    "        )\n",
    "\n",
    "    def forward(self, x): return self.net(x).view(-1)  # scalar per sample\n",
    "\n",
    "\n",
    "def gradient_penalty(critic, real, fake, device):\n",
    "    bsz = real.size(0)\n",
    "    eps = torch.rand(bsz, 1, 1, 1, device=device)\n",
    "    interp = eps * real + (1 - eps) * fake\n",
    "    interp.requires_grad_(True)\n",
    "    scores = critic(interp)\n",
    "    grads = torch.autograd.grad(\n",
    "        outputs=scores, inputs=interp,\n",
    "        grad_outputs=torch.ones_like(scores, device=device),\n",
    "        create_graph=True, retain_graph=True, only_inputs=True\n",
    "    )[0]\n",
    "    grads = grads.view(bsz, -1)\n",
    "    gp = ((grads.norm(2, dim=1) - 1) ** 2).mean()\n",
    "    return gp\n",
    "\n",
    "\n",
    "def get_wgan_data(name, image_size, batch_size, workers=4):\n",
    "    name = name.lower()\n",
    "    if name in {\"mnist\", \"fashion-mnist\"}:\n",
    "        nc = 1\n",
    "        tf = T.Compose([T.Resize(image_size), T.ToTensor(), T.Normalize((0.5,), (0.5,))])\n",
    "        ds = datasets.MNIST if name == \"mnist\" else datasets.FashionMNIST\n",
    "        train = ds(root=\"./data\", train=True, download=True, transform=tf)\n",
    "    elif name == \"cifar10\":\n",
    "        nc = 3\n",
    "        tf = T.Compose([T.Resize(image_size), T.ToTensor(), T.Normalize((0.5,)*3, (0.5,)*3)])\n",
    "        train = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=tf)\n",
    "    else:\n",
    "        raise ValueError(\"dataset must be mnist | fashion-mnist | cifar10\")\n",
    "\n",
    "    loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True,\n",
    "                                         num_workers=workers, pin_memory=True)\n",
    "    return loader, nc\n",
    "\n",
    "\n",
    "def train_wgan(args):\n",
    "    make_dirs()\n",
    "    seed_all(args.seed)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.cpu else \"cpu\")\n",
    "    print(\"Device:\", device)\n",
    "\n",
    "    loader, nc = get_wgan_data(args.dataset, args.image_size, args.batch_size, args.num_workers)\n",
    "    G = Gen(args.nz, args.ngf, nc).to(device)\n",
    "    C = Critic(nc, args.ndf).to(device)\n",
    "\n",
    "    optG = optim.Adam(G.parameters(), lr=args.lr, betas=(0.0, 0.9))\n",
    "    optC = optim.Adam(C.parameters(), lr=args.lr, betas=(0.0, 0.9))\n",
    "\n",
    "    fixed = torch.randn(64, args.nz, 1, 1, device=device)\n",
    "\n",
    "    step = 0\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        for i, (real, _) in enumerate(loader):\n",
    "            real = real.to(device)\n",
    "            bsz = real.size(0)\n",
    "\n",
    "            # update critic n_critic times\n",
    "            for _ in range(args.n_critic):\n",
    "                z = torch.randn(bsz, args.nz, 1, 1, device=device)\n",
    "                fake = G(z).detach()\n",
    "                optC.zero_grad(set_to_none=True)\n",
    "                loss_c = (C(fake).mean() - C(real).mean())\n",
    "                gp = gradient_penalty(C, real, fake, device)\n",
    "                total_c = loss_c + args.lambda_gp * gp\n",
    "                total_c.backward()\n",
    "                optC.step()\n",
    "\n",
    "            # update generator\n",
    "            z = torch.randn(bsz, args.nz, 1, 1, device=device)\n",
    "            optG.zero_grad(set_to_none=True)\n",
    "            fake = G(z)\n",
    "            # maximize critic(fake) -> minimize -mean\n",
    "            g_loss = -C(fake).mean()\n",
    "            g_loss.backward()\n",
    "            optG.step()\n",
    "\n",
    "            if (i + 1) % args.log_interval == 0:\n",
    "                print(f\"[E{epoch}/{args.epochs}] [B{i+1}/{len(loader)}] \"\n",
    "                      f\"C: {total_c.item():.3f} (gp {gp.item():.3f}) | G: {g_loss.item():.3f}\")\n",
    "\n",
    "            if step % args.sample_interval == 0:\n",
    "                with torch.no_grad():\n",
    "                    imgs = G(fixed).cpu()\n",
    "                grid = vutils.make_grid(imgs, nrow=8, normalize=True, value_range=(-1, 1))\n",
    "                vutils.save_image(grid, f\"samples_wgan/e{epoch:03d}_s{step:06d}.png\")\n",
    "            step += 1\n",
    "\n",
    "        torch.save(G.state_dict(), f\"checkpoints_wgan/G_e{epoch}.pt\")\n",
    "        torch.save(C.state_dict(), f\"checkpoints_wgan/C_e{epoch}.pt\")\n",
    "        print(f\"[Epoch {epoch}] checkpoints saved.\")\n",
    "\n",
    "\n",
    "def parse_wgan_args():\n",
    "    p = argparse.ArgumentParser(description=\"WGAN-GP from scratch (PyTorch).\")\n",
    "    p.add_argument(\"--dataset\", type=str, default=\"mnist\", choices=[\"mnist\", \"fashion-mnist\", \"cifar10\"])\n",
    "    p.add_argument(\"--image_size\", type=int, default=64)\n",
    "    p.add_argument(\"--epochs\", type=int, default=30)\n",
    "    p.add_argument(\"--batch_size\", type=int, default=128)\n",
    "    p.add_argument(\"--nz\", type=int, default=128)\n",
    "    p.add_argument(\"--ngf\", type=int, default=64)\n",
    "    p.add_argument(\"--ndf\", type=int, default=64)\n",
    "    p.add_argument(\"--lr\", type=float, default=2e-4)\n",
    "    p.add_argument(\"--lambda_gp\", type=float, default=10.0)\n",
    "    p.add_argument(\"--n_critic\", type=int, default=5)\n",
    "    p.add_argument(\"--num_workers\", type=int, default=4)\n",
    "    p.add_argument(\"--log_interval\", type=int, default=100)\n",
    "    p.add_argument(\"--sample_interval\", type=int, default=300)\n",
    "    p.add_argument(\"--seed\", type=int, default=42)\n",
    "    p.add_argument(\"--cpu\", action=\"store_true\")\n",
    "    return p.parse_args()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34af28f",
   "metadata": {},
   "source": [
    "# Vec2Vec GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330a8e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vec2vec_gan.py\n",
    "# Unpaired vector-to-vector translation with:\n",
    "# - Output-level adversarial (M1->M2 and M2->M1)\n",
    "# - Latent-level adversarial (A1 vs A2 after shared backbone T)\n",
    "# - Reconstruction (R1, R2)\n",
    "# - Cycle-consistency (F2(F1(x)) ~ x and F1(F2(y)) ~ y)\n",
    "# - Vector Space Preservation (VSP) on pairwise dot-products\n",
    "#\n",
    "# Works with:\n",
    "#   (A) real embeddings from .npy files (unpaired):  --data_m1 pathA.npy --data_m2 pathB.npy\n",
    "#   (B) synthetic toy data (default): mixtures of Gaussians in two spaces with an affine warp.\n",
    "#\n",
    "# Usage (toy):\n",
    "#   python vec2vec_gan.py --epochs 30 --batch_size 128\n",
    "#\n",
    "# Usage (real .npy):\n",
    "#   python vec2vec_gan.py --data_m1 m1.npy --data_m2 m2.npy --epochs 50 --dim 768\n",
    "#\n",
    "# Inference:\n",
    "#   After training, it prints a small demo translating a handful of vectors,\n",
    "#   along with cosine similarities before/after to illustrate geometry preservation.\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# Dataset (unpaired)\n",
    "# --------------------------\n",
    "class UnpairedEmbeddingDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    - If data_m1/m2 given: load .npy arrays of shape (N, d) and (M, d).\n",
    "    - n1/n2: number of samples to generate for each domain.\n",
    "    - mixtures: number of mixtures to generate for synthetic data.\n",
    "    - std: standard deviation of the normal distribution for synthetic data.\n",
    "    - Else: generate synthetic mixtures with known affine warp between spaces.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim: int, n1: int, n2: int,\n",
    "                 data_m1: Optional[str] = None,\n",
    "                 data_m2: Optional[str] = None,\n",
    "                 mixtures: int = 4,\n",
    "                 std: float = 0.6):\n",
    "        if data_m1 and data_m2:\n",
    "            X: np.ndarray = np.load(data_m1)  # shape (N, d)\n",
    "            Y: np.ndarray = np.load(data_m2)  # shape (M, d)\n",
    "            assert X.ndim == 2 and Y.ndim == 2, \"Loaded arrays must be 2D.\"\n",
    "            assert X.shape[1] == Y.shape[1], \"Both spaces must have same dimension.\"\n",
    "            self.X = torch.from_numpy(X).float()\n",
    "            self.Y = torch.from_numpy(Y).float()\n",
    "            self.dim = X.shape[1]\n",
    "        else:\n",
    "            # synthetic toy data\n",
    "            self.dim = dim\n",
    "            centers = torch.randn(mixtures, dim) * 3.0\n",
    "            # sample m1\n",
    "            idxs1 = torch.randint(0, mixtures, (n1,))\n",
    "            self.X = centers[idxs1] + std * torch.randn(n1, dim)\n",
    "\n",
    "            # create an affine mapping + nonlinearity to define m2 domain\n",
    "            W = torch.randn(dim, dim)\n",
    "            U, _, Vt = torch.linalg.svd(W, full_matrices=False)\n",
    "            R = (U @ Vt)  # orthogonal (rotation/reflection)\n",
    "            scale = 1.2\n",
    "            b = torch.randn(dim) * 0.2\n",
    "\n",
    "            # Slight nonlinearity: tanh on a few coords\n",
    "            Y_base = self.X @ (scale * R).T + b\n",
    "            Y_base[:, : dim // 6] = torch.tanh(Y_base[:, : dim // 6])\n",
    "\n",
    "            # re-sample independently for unpaired setting\n",
    "            idxs2 = torch.randint(0, mixtures, (n2,))\n",
    "            base2 = centers[idxs2] + std * torch.randn(n2, dim)\n",
    "            self.Y = base2 @ (scale * R).T + b\n",
    "            self.Y[:, : dim // 6] = torch.tanh(self.Y[:, : dim // 6])\n",
    "\n",
    "        # normalize to roughly unit norm (optional, helps numerics for dot/cos)\n",
    "        self.X = nn.functional.normalize(self.X, dim=-1)\n",
    "        self.Y = nn.functional.normalize(self.Y, dim=-1)\n",
    "\n",
    "    def __len__(self):\n",
    "        # Epoch length = max of both (like unpaired image datasets)\n",
    "        return max(len(self.X), len(self.Y))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.X[idx % len(self.X)]\n",
    "        y = self.Y[random.randint(0, len(self.Y) - 1)]\n",
    "        return x, y\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# Models\n",
    "# --------------------------\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, dims, act=nn.SiLU, layernorm=True, dropout=0.0):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for i in range(len(dims) - 1):\n",
    "            layers.append(nn.Linear(dims[i], dims[i+1]))\n",
    "            if i < len(dims) - 2:\n",
    "                if layernorm:\n",
    "                    layers.append(nn.LayerNorm(dims[i+1]))\n",
    "                layers.append(act())\n",
    "                if dropout > 0:\n",
    "                    layers.append(nn.Dropout(dropout))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class V2VResidualBlock(nn.Module):\n",
    "    def __init__(self, dim, hidden_mult=2, dropout=0.0):\n",
    "        super().__init__()\n",
    "        hid = dim * hidden_mult\n",
    "        self.f = nn.Sequential(\n",
    "            nn.Linear(dim, hid),\n",
    "            nn.LayerNorm(hid),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(dropout) if dropout > 0 else nn.Identity(),\n",
    "            nn.Linear(hid, dim),\n",
    "        )\n",
    "        self.n = nn.LayerNorm(dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.n(x + self.f(x))\n",
    "\n",
    "\n",
    "class V2VResidualBackbone(nn.Module):\n",
    "    def __init__(self, dim, depth=4, hidden_mult=2, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.blocks = nn.ModuleList([V2VResidualBlock(dim, hidden_mult, dropout) for _ in range(depth)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class AdapterIn(nn.Module):\n",
    "    # A1/A2: d -> z\n",
    "    def __init__(self, d, z):\n",
    "        super().__init__()\n",
    "        self.f = MLP([d, 2*z, z], act=nn.SiLU, layernorm=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.f(x)\n",
    "\n",
    "\n",
    "class AdapterOut(nn.Module):\n",
    "    # B1/B2: z -> d\n",
    "    def __init__(self, z, d):\n",
    "        super().__init__()\n",
    "        self.f = MLP([z, 2*z, d], act=nn.SiLU, layernorm=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.f(x)\n",
    "\n",
    "\n",
    "class DV2Viscriminator(nn.Module):\n",
    "    # Simple MLP with spectral norm on the first layers for stability\n",
    "    def __init__(self, in_dim, width=512, depth=3):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        d_prev = in_dim\n",
    "        for i in range(depth - 1):\n",
    "            lin = nn.utils.spectral_norm(nn.Linear(d_prev, width))\n",
    "            layers += [lin, nn.LeakyReLU(0.2, inplace=True)]\n",
    "            d_prev = width\n",
    "        layers.append(nn.utils.spectral_norm(nn.Linear(d_prev, 1)))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x).squeeze(-1)  # logits\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# Training helpers (losses)\n",
    "# --------------------------\n",
    "class Losses:\n",
    "    def __init__(self, lambda_rec=1.0, lambda_cc=10.0, lambda_vsp=1.0):\n",
    "        self.bce = nn.BCEWithLogitsLoss()\n",
    "        self.mse = nn.MSELoss()\n",
    "        self.lambda_rec = lambda_rec\n",
    "        self.lambda_cc = lambda_cc\n",
    "        self.lambda_vsp = lambda_vsp\n",
    "\n",
    "    def adv_d(self, D, real, fake):\n",
    "        # Discriminator loss: real->1, fake->0\n",
    "        return self.bce(D(real), torch.ones_like(real[:, 0] if real.ndim == 2 else real).float()) + \\\n",
    "               self.bce(D(fake.detach()), torch.zeros_like(fake[:, 0] if fake.ndim == 2 else fake).float())\n",
    "\n",
    "    def adv_g(self, D, fake):\n",
    "        # Generator (translator) wants fake->1\n",
    "        return self.bce(D(fake), torch.ones_like(fake[:, 0] if fake.ndim == 2 else fake).float())\n",
    "\n",
    "    def reconstruction(self, x, r1, y, r2):\n",
    "        return self.mse(r1, x) + self.mse(r2, y)\n",
    "\n",
    "    def cycle(self, x, x_cycled, y, y_cycled):\n",
    "        return self.mse(x_cycled, x) + self.mse(y_cycled, y)\n",
    "\n",
    "    def vsp(self, X, FX, Y, FY, max_subset: int = 64):\n",
    "        \"\"\"\n",
    "        Preserve pairwise dot products within a random subset (to keep O(B^2) manageable).\n",
    "        \"\"\"\n",
    "        def term(A, FA):\n",
    "            B = A.size(0)\n",
    "            if B > max_subset:\n",
    "                idx = torch.randperm(B, device=A.device)[:max_subset]\n",
    "                A = A[idx]\n",
    "                FA = FA[idx]\n",
    "            dots_src = A @ A.t()\n",
    "            dots_tgt = FA @ FA.t()\n",
    "            return ((dots_src - dots_tgt) ** 2).mean()\n",
    "\n",
    "        return term(X, FX) + term(Y, FY)\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# Full Model container\n",
    "# --------------------------\n",
    "class Vec2VecGAN(nn.Module):\n",
    "    \"\"\"\n",
    "    Vec2VecGAN model.\n",
    "    Args:\n",
    "        d: int, dimension of the input space.\n",
    "        z: int, dimension of the latent space.\n",
    "        back_depth: int, number of residual blocks in the backbone.\n",
    "        disc_width: int, width of the discriminator.\n",
    "        disc_depth: int, depth of the discriminator.\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "        d: int, \n",
    "        z: int,\n",
    "        back_depth=4,\n",
    "        disc_width=512, \n",
    "        disc_depth=3\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # Adapters and backbone\n",
    "        self.A1 = AdapterIn(d, z)\n",
    "        self.A2 = AdapterIn(d, z)\n",
    "        self.T = V2VResidualBackbone(z, depth=back_depth, hidden_mult=2)\n",
    "\n",
    "        self.B1 = AdapterOut(z, d)\n",
    "        self.B2 = AdapterOut(z, d)\n",
    "\n",
    "        # Output-space discriminators\n",
    "        self.D_M2 = Discriminator(d, width=disc_width, depth=disc_depth)  # judges F1 outputs against real M2\n",
    "        self.D_M1 = Discriminator(d, width=disc_width, depth=disc_depth)  # judges F2 outputs against real M1\n",
    "\n",
    "        # Latent discriminators (two symmetric ones)\n",
    "        self.DL_1 = Discriminator(z, width=disc_width, depth=disc_depth)  # real=z2, fake=z1\n",
    "        self.DL_2 = Discriminator(z, width=disc_width, depth=disc_depth)  # real=z1, fake=z2\n",
    "\n",
    "    # convenience\n",
    "    def encode1(self, x):  # M1 -> latent\n",
    "        return self.T(self.A1(x))\n",
    "\n",
    "    def encode2(self, y):  # M2 -> latent\n",
    "        return self.T(self.A2(y))\n",
    "\n",
    "    def F1(self, x):  # M1 -> M2\n",
    "        return self.B2(self.encode1(x))\n",
    "\n",
    "    def F2(self, y):  # M2 -> M1\n",
    "        return self.B1(self.encode2(y))\n",
    "\n",
    "    def R1(self, x):  # M1 -> latent -> M1\n",
    "        return self.B1(self.encode1(x))\n",
    "\n",
    "    def R2(self, y):  # M2 -> latent -> M2\n",
    "        return self.B2(self.encode2(y))\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# Training loop\n",
    "# --------------------------\n",
    "def train_v2c_gan(args):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.cpu else \"cpu\")\n",
    "    seed_all(args.seed)\n",
    "    ds = UnpairedEmbeddingDataset(\n",
    "        dim=args.dim, n1=args.n_m1, n2=args.n_m2,\n",
    "        data_m1=args.data_m1, data_m2=args.data_m2,\n",
    "        mixtures=args.mixtures, std=args.std\n",
    "    )\n",
    "    loader = torch.utils.data.DataLoader(\n",
    "        ds, batch_size=args.batch_size, shuffle=True, drop_last=True, num_workers=0\n",
    "    )\n",
    "\n",
    "    model = Vec2VecGAN(d=ds.dim, z=args.z, back_depth=args.back_depth,\n",
    "                       disc_width=args.disc_width, disc_depth=args.disc_depth).to(device)\n",
    "\n",
    "    # Two optimizers: one for discriminators, one for (A1,A2,T,B1,B2)\n",
    "    params_D = list(model.D_M1.parameters()) + list(model.D_M2.parameters()) + \\\n",
    "               list(model.DL_1.parameters()) + list(model.DL_2.parameters())\n",
    "    params_G = list(model.A1.parameters()) + list(model.A2.parameters()) + list(model.T.parameters()) + \\\n",
    "               list(model.B1.parameters()) + list(model.B2.parameters())\n",
    "\n",
    "    optD = optim.Adam(params_D, lr=args.lr_d, betas=(0.5, 0.999))\n",
    "    optG = optim.Adam(params_G, lr=args.lr_g, betas=(0.5, 0.999))\n",
    "\n",
    "    losses = Losses(lambda_rec=args.lambda_rec, lambda_cc=args.lambda_cc, lambda_vsp=args.lambda_vsp)\n",
    "\n",
    "    model.train()\n",
    "    global_step = 0\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        for i, (x, y) in enumerate(loader):\n",
    "            x = x.to(device)  # M1 batch\n",
    "            y = y.to(device)  # M2 batch\n",
    "\n",
    "            # ---- forward passes\n",
    "            z1 = model.encode1(x)       # M1 -> latent\n",
    "            z2 = model.encode2(y)       # M2 -> latent\n",
    "\n",
    "            f1 = model.B2(z1)           # M1 -> M2\n",
    "            f2 = model.B1(z2)           # M2 -> M1\n",
    "\n",
    "            r1 = model.B1(z1)           # M1 -> M1\n",
    "            r2 = model.B2(z2)           # M2 -> M2\n",
    "\n",
    "            x_cyc = model.F2(f1)        # M1 -> M2 -> M1\n",
    "            y_cyc = model.F1(f2)        # M2 -> M1 -> M2\n",
    "\n",
    "            # ---- 1) Discriminators\n",
    "            optD.zero_grad(set_to_none=True)\n",
    "\n",
    "            # Output-level D: D_M2 (real=y, fake=f1), D_M1 (real=x, fake=f2)\n",
    "            d_m2 = losses.adv_d(model.D_M2, y, f1)\n",
    "            d_m1 = losses.adv_d(model.D_M1, x, f2)\n",
    "\n",
    "            # Latent-level D: DL_1 (real=z2, fake=z1), DL_2 (real=z1, fake=z2)\n",
    "            d_l1 = losses.adv_d(model.DL_1, z2, z1)\n",
    "            d_l2 = losses.adv_d(model.DL_2, z1, z2)\n",
    "\n",
    "            d_total = d_m1 + d_m2 + d_l1 + d_l2\n",
    "            d_total.backward()\n",
    "            optD.step()\n",
    "\n",
    "            # ---- 2) Generators (adapters + backbone)\n",
    "            optG.zero_grad(set_to_none=True)\n",
    "\n",
    "            # refresh logits (avoid reusing detached fakes for G)\n",
    "            z1 = model.encode1(x)\n",
    "            z2 = model.encode2(y)\n",
    "            f1 = model.B2(z1)\n",
    "            f2 = model.B1(z2)\n",
    "\n",
    "            # adversarial G-side: make fakes look real; make both latents \"look like\" the other's\n",
    "            g_adv = losses.adv_g(model.D_M2, f1) + losses.adv_g(model.D_M1, f2) + \\\n",
    "                    losses.adv_g(model.DL_1, z1) + losses.adv_g(model.DL_2, z2)\n",
    "\n",
    "            # reconstruction and cycle\n",
    "            r1 = model.B1(z1)\n",
    "            r2 = model.B2(z2)\n",
    "            x_cyc = model.F2(f1)\n",
    "            y_cyc = model.F1(f2)\n",
    "\n",
    "            g_rec = losses.reconstruction(x, r1, y, r2)\n",
    "            g_cc  = losses.cycle(x, x_cyc, y, y_cyc)\n",
    "            g_vsp = losses.vsp(x, f1, y, f2, max_subset=args.vsp_subset)\n",
    "\n",
    "            g_total = g_adv + losses.lambda_rec * g_rec + losses.lambda_cc * g_cc + losses.lambda_vsp * g_vsp\n",
    "            g_total.backward()\n",
    "            optG.step()\n",
    "\n",
    "            if (i + 1) % args.log_interval == 0:\n",
    "                print(f\"[E{epoch:03d} B{i+1:04d}] \"\n",
    "                      f\"D: {d_total.item():.3f} | \"\n",
    "                      f\"G_adv: {g_adv.item():.3f} | \"\n",
    "                      f\"Rec: {g_rec.item():.3f} | \"\n",
    "                      f\"CC: {g_cc.item():.3f} | \"\n",
    "                      f\"VSP: {g_vsp.item():.3f}\")\n",
    "\n",
    "            global_step += 1\n",
    "\n",
    "        # save a light checkpoint each epoch\n",
    "        if args.save_ckpt:\n",
    "            Path(\"checkpoints\").mkdir(exist_ok=True)\n",
    "            torch.save(model.state_dict(), f\"checkpoints/vec2vec_e{epoch}.pt\")\n",
    "\n",
    "    return model, ds\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# Inference / demo\n",
    "# --------------------------\n",
    "def demo_inference(model: Vec2VecGAN, ds: UnpairedEmbeddingDataset, k: int = 5, device: str = \"cpu\"):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # pick k samples from M1, translate to M2\n",
    "        idx = torch.randint(0, len(ds.X), (k,))\n",
    "        x = ds.X[idx].to(device)\n",
    "        f1 = model.F1(x)\n",
    "\n",
    "        # compute average cosine between x and its reconstruction R1\n",
    "        r1 = model.R1(x)\n",
    "        cos_rec = cosine_sim(x, r1).mean().item()\n",
    "\n",
    "        # compute a rough neighborhood preservation score:\n",
    "        # cosine between pairwise distances before/after (k x k)\n",
    "        x_dots = (x @ x.t()).flatten()\n",
    "        f_dots = (f1 @ f1.t()).flatten()\n",
    "        corr = torch.corrcoef(torch.stack([x_dots, f_dots]))[0, 1].item()\n",
    "\n",
    "        print(\"\\n--- Inference Demo (M1 -> M2) ---\")\n",
    "        print(f\"Mean cosine(x, R1(x)) ≈ {cos_rec:.3f}  (reconstruction quality in M1)\")\n",
    "        print(f\"Correlation of pairwise dots (x vs F1(x)) ≈ {corr:.3f}  (geometry preservation)\")\n",
    "\n",
    "        # Show a couple of individual cosines between F2(F1(x)) and x (cycle)\n",
    "        x_cyc = model.F2(f1)\n",
    "        cyc_cos = cosine_sim(x, x_cyc)\n",
    "        print(\"Cosine(x, F2(F1(x))) for a few samples:\", [f\"{c.item():.3f}\" for c in cyc_cos])\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# CLI\n",
    "# --------------------------\n",
    "def parse_args():\n",
    "    p = argparse.ArgumentParser(description=\"Unpaired vec2vec GAN with Reconstruction, Cycle and VSP.\")\n",
    "    # data\n",
    "    p.add_argument(\"--data_m1\", type=str, default=\"\", help=\".npy file with M1 embeddings (N,d)\")\n",
    "    p.add_argument(\"--data_m2\", type=str, default=\"\", help=\".npy file with M2 embeddings (M,d)\")\n",
    "    p.add_argument(\"--dim\", type=int, default=128, help=\"embedding dimension (toy data)\")\n",
    "    p.add_argument(\"--n_m1\", type=int, default=20000)\n",
    "    p.add_argument(\"--n_m2\", type=int, default=20000)\n",
    "    p.add_argument(\"--mixtures\", type=int, default=6, help=\"num mixture centers for toy data\")\n",
    "    p.add_argument(\"--std\", type=float, default=0.6, help=\"cluster std for toy data\")\n",
    "    # model\n",
    "    p.add_argument(\"--z\", type=int, default=128, help=\"shared latent dim\")\n",
    "    p.add_argument(\"--back_depth\", type=int, default=4)\n",
    "    p.add_argument(\"--disc_width\", type=int, default=512)\n",
    "    p.add_argument(\"--disc_depth\", type=int, default=3)\n",
    "    # train\n",
    "    p.add_argument(\"--epochs\", type=int, default=20)\n",
    "    p.add_argument(\"--batch_size\", type=int, default=128)\n",
    "    p.add_argument(\"--lr_g\", type=float, default=2e-4)\n",
    "    p.add_argument(\"--lr_d\", type=float, default=2e-4)\n",
    "    p.add_argument(\"--lambda_rec\", type=float, default=1.0)\n",
    "    p.add_argument(\"--lambda_cc\", type=float, default=10.0)\n",
    "    p.add_argument(\"--lambda_vsp\", type=float, default=1.0)\n",
    "    p.add_argument(\"--vsp_subset\", type=int, default=64, help=\"subset size for VSP BxB dot loss\")\n",
    "    p.add_argument(\"--log_interval\", type=int, default=100)\n",
    "    p.add_argument(\"--save_ckpt\", action=\"store_true\")\n",
    "    # misc\n",
    "    p.add_argument(\"--cpu\", action=\"store_true\")\n",
    "    p.add_argument(\"--seed\", type=int, default=42)\n",
    "    return p.parse_args()\n",
    "\n",
    "\n",
    "# args = parse_args()\n",
    "# # if real data paths are provided, trust their dim\n",
    "# if args.data_m1 and args.data_m2:\n",
    "#     assert os.path.exists(args.data_m1) and os.path.exists(args.data_m2), \"npy paths not found.\"\n",
    "# model, ds = train_v2c_gan(args)\n",
    "# dev = next(model.parameters()).device\n",
    "# demo_inference(model, ds, k=8, device=dev)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8689e131",
   "metadata": {},
   "source": [
    "## TO-vec2vec-gan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ccd686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File 1 — `to_v2vgan_train.py`\n",
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Task-Operator vec2vec GAN (TO-v2vGAN)\n",
    "-------------------------------------\n",
    "Self-contained PyTorch training script that implements:\n",
    "  • Output-level adversarial losses (X->Y, Y->X)\n",
    "  • Latent-level adversarial losses (push shared latent alignment)\n",
    "  • Reconstruction, Cycle-consistency, Vector-Space Preservation (VSP)\n",
    "  • Operator prior: T(z) = P z + O(z) with near-orthogonal P and low-rank O (via bottleneck)\n",
    "Supports: synthetic toy data or .npy embedding files for X (descriptions) and Y (models).\n",
    "\n",
    "Usage examples\n",
    "--------------\n",
    "# synthetic quick run\n",
    "python to_v2vgan_train.py --epochs 20 --batch_size 128 --z 128 --dim 128\n",
    "\n",
    "# real embedding files (unpaired)\n",
    "python to_v2vgan_train.py --data_x desc.npy --data_y model.npy --epochs 50 --z 256\n",
    "\n",
    "Outputs\n",
    "-------\n",
    "checkpoints/vec2vec_e{epoch}.pt  (full model state)\n",
    "logs/metrics.csv                  (CSV log)\n",
    "\n",
    "Notes\n",
    "-----\n",
    "• This script is a clean consolidation of the components discussed in the paper template.\n",
    "• For brevity, we use BCE-GAN (you can flip to WGAN-GP if preferred).\n",
    "• Low-rank prior on O is encouraged via a bottleneck r << z plus L2; swap in spectral/nuclear\n",
    "  surrogates if you need stronger control.\n",
    "\"\"\"\n",
    "\n",
    "# ---------------------\n",
    "# Utils\n",
    "# ---------------------\n",
    "\n",
    "def ensure_dirs():\n",
    "    Path(\"checkpoints\").mkdir(exist_ok=True)\n",
    "    Path(\"logs\").mkdir(exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "def info_nce(anchor: torch.Tensor, positives: torch.Tensor, tau: float = 0.07) -> torch.Tensor:\n",
    "    # anchor: (B, d), positives: (B, d)\n",
    "    a = anchor / (anchor.norm(dim=-1, keepdim=True) + 1e-8)\n",
    "    p = positives / (positives.norm(dim=-1, keepdim=True) + 1e-8)\n",
    "    logits = a @ p.t() / tau  # (B, B)\n",
    "    targets = torch.arange(anchor.size(0), device=anchor.device)\n",
    "    return nn.CrossEntropyLoss()(logits, targets)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Datasets\n",
    "# -----------------------------\n",
    "class UnpairedEmbeddingDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Unpaired X and Y embeddings.\n",
    "    - If data_x/data_y are paths to .npy, load them.\n",
    "    - Else generate synthetic mixtures for X, and map to Y by an affine+nonlinear warp,\n",
    "      but re-sample unpaired so X and Y are not aligned index-wise.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 dim: int,\n",
    "                 n_x: int,\n",
    "                 n_y: int,\n",
    "                 data_x: Optional[str] = None,\n",
    "                 data_y: Optional[str] = None,\n",
    "                 mixtures: int = 6,\n",
    "                 std: float = 0.6):\n",
    "        if data_x and data_y:\n",
    "            X = np.load(data_x)\n",
    "            Y = np.load(data_y)\n",
    "            assert X.ndim == 2 and Y.ndim == 2, \"npy arrays must be 2D (N,d)\"\n",
    "            assert X.shape[1] == Y.shape[1] or dim == X.shape[1] == Y.shape[1], \"dim mismatch\"\n",
    "            self.dim = X.shape[1]\n",
    "            self.X = torch.from_numpy(X).float()\n",
    "            self.Y = torch.from_numpy(Y).float()\n",
    "        else:\n",
    "            self.dim = dim\n",
    "            centers = torch.randn(mixtures, dim) * 3.0\n",
    "            # X\n",
    "            idxs_x = torch.randint(0, mixtures, (n_x,))\n",
    "            X = centers[idxs_x] + std * torch.randn(n_x, dim)\n",
    "\n",
    "            # define Y transform\n",
    "            W = torch.randn(dim, dim)\n",
    "            U, _, Vt = torch.linalg.svd(W, full_matrices=False)\n",
    "            R = U @ Vt  # orthogonal\n",
    "            scale = 1.15\n",
    "            b = torch.randn(dim) * 0.2\n",
    "\n",
    "            # unpaired resample for Y\n",
    "            idxs_y = torch.randint(0, mixtures, (n_y,))\n",
    "            Y = centers[idxs_y] + std * torch.randn(n_y, dim)\n",
    "            Y = Y @ (scale * R).T + b\n",
    "            Y[:, : dim // 6] = torch.tanh(Y[:, : dim // 6])\n",
    "\n",
    "            self.X = X\n",
    "            self.Y = Y\n",
    "\n",
    "        # normalize (helps dot/cos stability)\n",
    "        self.X = nn.functional.normalize(self.X, dim=-1)\n",
    "        self.Y = nn.functional.normalize(self.Y, dim=-1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return max(len(self.X), len(self.Y))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.X[idx % len(self.X)]\n",
    "        y = self.Y[random.randint(0, len(self.Y) - 1)]\n",
    "        return x, y\n",
    "\n",
    "\n",
    "class PairedEmbeddingDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Optional paired anchors (X_paired, Y_paired) — same length, same dim.\n",
    "    \"\"\"\n",
    "    def __init__(self, paired_x: str, paired_y: str):\n",
    "        Xp = np.load(paired_x)\n",
    "        Yp = np.load(paired_y)\n",
    "        assert Xp.ndim == 2 and Yp.ndim == 2 and Xp.shape[0] == Yp.shape[0], \"paired arrays must match\"\n",
    "        assert Xp.shape[1] == Yp.shape[1], \"dim mismatch in paired arrays\"\n",
    "        self.Xp = nn.functional.normalize(torch.from_numpy(Xp).float(), dim=-1)\n",
    "        self.Yp = nn.functional.normalize(torch.from_numpy(Yp).float(), dim=-1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.Xp.size(0)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.Xp[idx], self.Yp[idx]\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Modules\n",
    "# -----------------------------\n",
    "class TOVMLP(nn.Module):\n",
    "    def __init__(self, dims, act=nn.SiLU, layernorm=True, dropout=0.0):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for i in range(len(dims) - 1):\n",
    "            layers.append(nn.Linear(dims[i], dims[i+1]))\n",
    "            if i < len(dims) - 2:\n",
    "                if layernorm:\n",
    "                    layers.append(nn.LayerNorm(dims[i+1]))\n",
    "                layers.append(act())\n",
    "                if dropout > 0:\n",
    "                    layers.append(nn.Dropout(dropout))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x): return self.net(x)\n",
    "\n",
    "\n",
    "class TOVAdapterIn(nn.Module):\n",
    "    # A_x, A_y: d -> z\n",
    "    def __init__(self, d, z):\n",
    "        super().__init__()\n",
    "        self.f = MLP([d, 2*z, z], act=nn.SiLU, layernorm=True)\n",
    "\n",
    "    def forward(self, x): return self.f(x)\n",
    "\n",
    "\n",
    "class TOVAdapterOut(nn.Module):\n",
    "    # B_x, B_y: z -> d\n",
    "    def __init__(self, z, d):\n",
    "        super().__init__()\n",
    "        self.f = MLP([z, 2*z, d], act=nn.SiLU, layernorm=True)\n",
    "\n",
    "    def forward(self, z): return self.f(z)\n",
    "\n",
    "\n",
    "class TaskOperatorResidual(nn.Module):\n",
    "    \"\"\"\n",
    "    O(z): low-rank residual via bottleneck r << zdim\n",
    "    \"\"\"\n",
    "    def __init__(self, zdim: int, rank: int):\n",
    "        super().__init__()\n",
    "        rank = max(1, min(rank, zdim))\n",
    "        self.f = nn.Sequential(\n",
    "            nn.Linear(zdim, rank, bias=False),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(rank, zdim, bias=False),\n",
    "        )\n",
    "\n",
    "    def forward(self, z): return self.f(z)\n",
    "\n",
    "\n",
    "class BackboneTO(nn.Module):\n",
    "    \"\"\"\n",
    "    T(z) = P z + O(z)\n",
    "    P: linear, near-orthogonal; O: low-rank residual (task operator)\n",
    "    \"\"\"\n",
    "    def __init__(self, zdim: int, rank: int):\n",
    "        super().__init__()\n",
    "        self.P = nn.Linear(zdim, zdim, bias=False)  # will regularize towards orthogonal\n",
    "        nn.init.eye_(self.P.weight)                 # start near identity\n",
    "        self.O = TaskOperatorResidual(zdim, rank)\n",
    "        self.norm = nn.LayerNorm(zdim)\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.norm(self.P(z) + self.O(z))\n",
    "\n",
    "\n",
    "class TOVDiscriminator(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple spectral-normalized MLP critic (logits).\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim: int, width: int = 512, depth: int = 3):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        d = in_dim\n",
    "        for _ in range(depth - 1):\n",
    "            lin = nn.utils.spectral_norm(nn.Linear(d, width))\n",
    "            layers += [lin, nn.LeakyReLU(0.2, inplace=True)]\n",
    "            d = width\n",
    "        layers.append(nn.utils.spectral_norm(nn.Linear(d, 1)))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor: return self.net(x).squeeze(-1)  # logits\n",
    "\n",
    "\n",
    "class TOv2vGAN(nn.Module):\n",
    "    \"\"\"\n",
    "    Full model container with adapters, backbone (P+O), and discriminators.\n",
    "    \"\"\"\n",
    "    def __init__(self, d: int, z: int, rank_o: int,\n",
    "                 disc_width=512, disc_depth=3):\n",
    "        super().__init__()\n",
    "        # adapters\n",
    "        self.Ax = TOVAdapterIn(d, z)\n",
    "        self.Ay = TOVAdapterIn(d, z)\n",
    "        # backbone\n",
    "        self.T = BackboneTO(zdim=z, rank=rank_o)\n",
    "        # outputs\n",
    "        self.Bx = TOVAdapterOut(z, d)\n",
    "        self.By = TOVAdapterOut(z, d)\n",
    "        # discriminators\n",
    "        self.Dy = TOVDiscriminator(d, width=disc_width, depth=disc_depth)  # real y vs F(x)\n",
    "        self.Dx = TOVDiscriminator(d, width=disc_width, depth=disc_depth)  # real x vs G(y)\n",
    "        self.Dl1 = TOVDiscriminator(z, width=disc_width, depth=disc_depth) # real z_y vs fake z_x\n",
    "        self.Dl2 = TOVDiscriminator(z, width=disc_width, depth=disc_depth) # real z_x vs fake z_y\n",
    "\n",
    "    # encoders\n",
    "    def enc_x(self, x): return self.T(self.Ax(x))\n",
    "    def enc_y(self, y): return self.T(self.Ay(y))\n",
    "\n",
    "    # translators\n",
    "    def F_xy(self, x): return self.By(self.enc_x(x))\n",
    "    def F_yx(self, y): return self.Bx(self.enc_y(y))\n",
    "\n",
    "    # recon\n",
    "    def R_x(self, x): return self.Bx(self.enc_x(x))\n",
    "    def R_y(self, y): return self.By(self.enc_y(y))\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Losses\n",
    "# -----------------------------\n",
    "class LossPack:\n",
    "    def __init__(self,\n",
    "                 lambda_rec=1.0,\n",
    "                 lambda_cyc=10.0,\n",
    "                 lambda_vsp=1.0,\n",
    "                 lambda_ortho=0.1,\n",
    "                 lambda_energy=0.01,\n",
    "                 lambda_decomp=0.0,\n",
    "                 lambda_nce=0.0,\n",
    "                 lambda_l2=0.0,\n",
    "                 vsp_subset=64):\n",
    "        self.bce = nn.BCEWithLogitsLoss()\n",
    "        self.mse = nn.MSELoss()\n",
    "        self.huber = nn.SmoothL1Loss(beta=1.0)\n",
    "        self.lambda_rec = lambda_rec\n",
    "        self.lambda_cyc = lambda_cyc\n",
    "        self.lambda_vsp = lambda_vsp\n",
    "        self.lambda_ortho = lambda_ortho\n",
    "        self.lambda_energy = lambda_energy\n",
    "        self.lambda_decomp = lambda_decomp\n",
    "        self.lambda_nce = lambda_nce\n",
    "        self.lambda_l2 = lambda_l2\n",
    "        self.vsp_subset = vsp_subset\n",
    "\n",
    "    # Adversarial (D)\n",
    "    def adv_d(self, D, real, fake):\n",
    "        # label smoothing for real\n",
    "        ones = torch.full((real.size(0),), 0.9, device=real.device)\n",
    "        zeros = torch.zeros(fake.size(0), device=fake.device)\n",
    "        return self.bce(D(real), ones) + self.bce(D(fake.detach()), zeros)\n",
    "\n",
    "    # Adversarial (G)\n",
    "    def adv_g(self, D, fake):\n",
    "        ones = torch.ones(fake.size(0), device=fake.device)\n",
    "        return self.bce(D(fake), ones)\n",
    "\n",
    "    # Reconstruction\n",
    "    def rec(self, x, rx, y, ry):\n",
    "        return self.mse(rx, x) + self.mse(ry, y)\n",
    "\n",
    "    # Cycle\n",
    "    def cyc(self, x, x_cyc, y, y_cyc):\n",
    "        return self.mse(x_cyc, x) + self.mse(y_cyc, y)\n",
    "\n",
    "    # VSP\n",
    "    def vsp(self, X, FX, Y, FY):\n",
    "        def term(A, FA):\n",
    "            if A.size(0) > self.vsp_subset:\n",
    "                idx = torch.randperm(A.size(0), device=A.device)[:self.vsp_subset]\n",
    "                A = A[idx]\n",
    "                FA = FA[idx]\n",
    "            return ((pairwise_dot(A) - pairwise_dot(FA)) ** 2).mean()\n",
    "        return term(X, FX) + term(Y, FY)\n",
    "\n",
    "    # Operator regularizers\n",
    "    def op_losses(self, model: TOv2vGAN, Ax_x: torch.Tensor, Ay_y: torch.Tensor,\n",
    "              ByPxAx: torch.Tensor, ByOA: torch.Tensor):\n",
    "        P = model.T.P.weight  # (z,z)\n",
    "        Id = torch.eye(P.size(0), device=P.device)\n",
    "\n",
    "        # 1) P near-orthogonal (near-isometry)\n",
    "        L_ortho = torch.norm(P.T @ P - Id, p='fro') ** 2\n",
    "\n",
    "        # 2) O should have small energy on typical latents\n",
    "        Ozx = model.T.O(Ax_x)\n",
    "        Ozy = model.T.O(Ay_y)\n",
    "        L_energy = (Ozx.pow(2).sum(dim=-1).mean() + Ozy.pow(2).sum(dim=-1).mean())\n",
    "\n",
    "        # 3) Optional additive decomposition in Y-space:\n",
    "        #    F_xy(x) ≈ By(P(Ax(x))) + By(O(Ax(x)))\n",
    "        #    (Use T.norm so this matches the actual forward path.)\n",
    "        L_decomp = torch.tensor(0.0, device=Ax_x.device)\n",
    "        if self.lambda_decomp > 0:\n",
    "            z_sum = model.T.P(Ax_x) + model.T.O(Ax_x)\n",
    "            z_sum = model.T.norm(z_sum)              # matches T.forward's LayerNorm\n",
    "            y_full = model.By(z_sum)                 # F_{x->y}(x)\n",
    "            y_add  = ByPxAx + ByOA                   # By(P(Ax(x))) + By(O(Ax(x)))\n",
    "            L_decomp = nn.MSELoss()(y_full, y_add)\n",
    "\n",
    "        return (\n",
    "            self.lambda_ortho * L_ortho\n",
    "            + self.lambda_energy * L_energy\n",
    "            + self.lambda_decomp * L_decomp\n",
    "        )\n",
    "\n",
    "\n",
    "    # Paired tethers\n",
    "    def paired_losses(self, x_p: torch.Tensor, y_p: torch.Tensor, yhat_p: torch.Tensor):\n",
    "        L = 0.0\n",
    "        if self.lambda_nce > 0:\n",
    "            L += self.lambda_nce * info_nce(yhat_p, y_p)\n",
    "        if self.lambda_l2 > 0:\n",
    "            L += self.lambda_l2 * self.huber(yhat_p, y_p)\n",
    "        return L\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Training\n",
    "# -----------------------------\n",
    "def make_loader(dataset, batch_size):\n",
    "    return torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True, num_workers=0)\n",
    "\n",
    "\n",
    "def next_batch(it, loader):\n",
    "    try:\n",
    "        batch = next(it)\n",
    "    except StopIteration:\n",
    "        it = iter(loader)\n",
    "        batch = next(it)\n",
    "    return batch, it\n",
    "\n",
    "\n",
    "def train(args):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.cpu else \"cpu\")\n",
    "    seed_all(args.seed)\n",
    "\n",
    "    # Data\n",
    "    ds_u = UnpairedEmbeddingDataset(\n",
    "        dim=args.dim, n_x=args.n_x, n_y=args.n_y,\n",
    "        data_x=args.data_x if args.data_x else None,\n",
    "        data_y=args.data_y if args.data_y else None,\n",
    "        mixtures=args.mixtures, std=args.std\n",
    "    )\n",
    "    loader_u = make_loader(ds_u, args.batch_size)\n",
    "    it_u = iter(loader_u)\n",
    "\n",
    "    ds_p = None\n",
    "    loader_p = None\n",
    "    it_p = None\n",
    "    if args.paired_x and args.paired_y:\n",
    "        ds_p = PairedEmbeddingDataset(args.paired_x, args.paired_y)\n",
    "        loader_p = make_loader(ds_p, min(args.batch_size, len(ds_p)))\n",
    "        it_p = iter(loader_p)\n",
    "\n",
    "    d = ds_u.X.shape[1]  # embedding dim\n",
    "    model = TOv2vGAN(d=d, z=args.z, rank_o=args.rank_o, disc_width=args.disc_width, disc_depth=args.disc_depth).to(device)\n",
    "\n",
    "    # Params\n",
    "    params_D = list(model.Dx.parameters()) + list(model.Dy.parameters()) + list(model.Dl1.parameters()) + list(model.Dl2.parameters())\n",
    "    params_G = list(model.Ax.parameters()) + list(model.Ay.parameters()) + list(model.T.parameters()) + list(model.Bx.parameters()) + list(model.By.parameters())\n",
    "\n",
    "    optD = optim.Adam(params_D, lr=args.lr_d, betas=(0.5, 0.999))\n",
    "    optG = optim.Adam(params_G, lr=args.lr_g, betas=(0.5, 0.999))\n",
    "\n",
    "    losses = LossPack(lambda_rec=args.lambda_rec, lambda_cyc=args.lambda_cyc, lambda_vsp=args.lambda_vsp,\n",
    "                      lambda_ortho=args.lambda_ortho, lambda_energy=args.lambda_energy,\n",
    "                      lambda_decomp=0.0, lambda_nce=args.lambda_nce, lambda_l2=args.lambda_l2,\n",
    "                      vsp_subset=args.vsp_subset)\n",
    "\n",
    "    global_step = 0\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        for _ in range(len(loader_u)):\n",
    "            (x_u, y_u), it_u = next_batch(it_u, loader_u)\n",
    "            x_u = x_u.to(device)\n",
    "            y_u = y_u.to(device)\n",
    "\n",
    "            # ---- Forward (unpaired)\n",
    "            zx = model.enc_x(x_u)   # z_x\n",
    "            zy = model.enc_y(y_u)   # z_y\n",
    "            y_hat = model.By(zx)    # F_{x->y}(x)\n",
    "            x_hat = model.Bx(zy)    # F_{y->x}(y)\n",
    "            rx = model.Bx(zx)       # R_x(x)\n",
    "            ry = model.By(zy)       # R_y(y)\n",
    "\n",
    "            x_cyc = model.F_yx(y_hat)  # x -> y -> x\n",
    "            y_cyc = model.F_xy(x_hat)  # y -> x -> y\n",
    "\n",
    "            # ---- 1) Discriminators\n",
    "            optD.zero_grad(set_to_none=True)\n",
    "            d_y = losses.adv_d(model.Dy, y_u, y_hat)  # real y vs fake y_hat\n",
    "            d_x = losses.adv_d(model.Dx, x_u, x_hat)  # real x vs fake x_hat\n",
    "            d_l1 = losses.adv_d(model.Dl1, zy, zx)    # latent: real=zy, fake=zx\n",
    "            d_l2 = losses.adv_d(model.Dl2, zx, zy)    # latent: real=zx, fake=zy\n",
    "            d_total = d_y + d_x + d_l1 + d_l2\n",
    "            d_total.backward()\n",
    "            optD.step()\n",
    "\n",
    "            # ---- 2) Generators (adapters/backbone)\n",
    "            optG.zero_grad(set_to_none=True)\n",
    "            # refresh forward for generator grads\n",
    "            zx = model.enc_x(x_u)\n",
    "            zy = model.enc_y(y_u)\n",
    "            y_hat = model.By(zx)\n",
    "            x_hat = model.Bx(zy)\n",
    "            rx = model.Bx(zx)\n",
    "            ry = model.By(zy)\n",
    "            x_cyc = model.F_yx(y_hat)\n",
    "            y_cyc = model.F_xy(x_hat)\n",
    "\n",
    "            # adversarial\n",
    "            g_adv = (losses.adv_g(model.Dy, y_hat) + losses.adv_g(model.Dx, x_hat) +\n",
    "                     losses.adv_g(model.Dl1, zx) + losses.adv_g(model.Dl2, zy))\n",
    "            # structural\n",
    "            g_rec = losses.rec(x_u, rx, y_u, ry)\n",
    "            g_cyc = losses.cyc(x_u, x_cyc, y_u, y_cyc)\n",
    "            g_vsp = losses.vsp(x_u, y_hat, y_u, x_hat)\n",
    "\n",
    "            # operator regularizers (need Ax(x), Ay(y))\n",
    "            ax = model.Ax(x_u)\n",
    "            ay = model.Ay(y_u)\n",
    "            ByPxAx = model.By(model.T.P(ax))\n",
    "            ByOA   = model.By(model.T.O(ax))\n",
    "            g_op = losses.op_losses(model, ax, ay, ByPxAx, ByOA)\n",
    "\n",
    "            # paired tethers (optional)\n",
    "            g_pair = 0.0\n",
    "            if ds_p is not None and (losses.lambda_nce > 0 or losses.lambda_l2 > 0):\n",
    "                (xp, yp), it_p = next_batch(it_p, loader_p)\n",
    "                xp = xp.to(device)\n",
    "                yp = yp.to(device)\n",
    "                yhat_p = model.F_xy(xp)\n",
    "                g_pair = losses.paired_losses(xp, yp, yhat_p)\n",
    "\n",
    "            g_total = g_adv + losses.lambda_rec * g_rec + losses.lambda_cyc * g_cyc + losses.lambda_vsp * g_vsp + g_op + g_pair\n",
    "            g_total.backward()\n",
    "            nn.utils.clip_grad_norm_(params_G, max_norm=5.0)\n",
    "            optG.step()\n",
    "\n",
    "            # occasional re-orthogonalization of P (project to closest orthogonal)\n",
    "            if args.reortho_every > 0 and (global_step + 1) % args.reortho_every == 0:\n",
    "                with torch.no_grad():\n",
    "                    W = model.T.P.weight.data\n",
    "                    U, _, Vt = torch.linalg.svd(W, full_matrices=False)\n",
    "                    model.T.P.weight.copy_(U @ Vt)\n",
    "\n",
    "            # logs\n",
    "            if (global_step + 1) % args.log_interval == 0:\n",
    "                print(f\"[E{epoch:03d} S{global_step+1:06d}] \"\n",
    "                      f\"D:{d_total.item():.3f} | G_adv:{g_adv.item():.3f} \"\n",
    "                      f\"| Rec:{g_rec.item():.3f} Cyc:{g_cyc.item():.3f} VSP:{g_vsp.item():.3f} \"\n",
    "                      f\"| Op:{g_op.item():.3f} Pair:{(g_pair if isinstance(g_pair,float) else g_pair.item()):.3f}\")\n",
    "\n",
    "            global_step += 1\n",
    "\n",
    "        # save ckpt\n",
    "        if args.save_ckpt:\n",
    "            Path(\"checkpoints\").mkdir(exist_ok=True)\n",
    "            torch.save(model.state_dict(), f\"checkpoints/to_v2v_e{epoch}.pt\")\n",
    "\n",
    "    return model, ds_u\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Inference / Demo\n",
    "# -----------------------------\n",
    "def demo(model: TOv2vGAN, ds: UnpairedEmbeddingDataset, k: int = 8, device: str = \"cpu\"):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # sample k queries from X, translate to Y\n",
    "        idx = torch.randint(0, ds.X.size(0), (k,))\n",
    "        xq = ds.X[idx].to(device)\n",
    "        y_hat = model.F_xy(xq)\n",
    "\n",
    "        # recon / cycle quality (in X)\n",
    "        rx = model.R_x(xq)\n",
    "        xcyc = model.F_yx(y_hat)\n",
    "        rec_cos = cosine_sim(xq, rx).mean().item()\n",
    "        cyc_cos = cosine_sim(xq, xcyc).mean().item()\n",
    "\n",
    "        # neighborhood preservation: corr of pairwise dot matrices (k x k)\n",
    "        x_dots = pairwise_dot(xq).flatten()\n",
    "        yh_dots = pairwise_dot(y_hat).flatten()\n",
    "        corr = torch.corrcoef(torch.stack([x_dots, yh_dots]))[0, 1].item()\n",
    "\n",
    "        # retrieval demo: find nearest Y for each translated y_hat (cosine top-1)\n",
    "        Yall = ds.Y.to(device)\n",
    "        yhat_n = y_hat / (y_hat.norm(dim=-1, keepdim=True) + 1e-8)\n",
    "        Yn = Yall / (Yall.norm(dim=-1, keepdim=True) + 1e-8)\n",
    "        sims = yhat_n @ Yn.t()  # (k, |Y|)\n",
    "        top1 = sims.argmax(dim=1).tolist()\n",
    "\n",
    "        print(\"\\n=== Inference Demo (X -> Y) ===\")\n",
    "        print(f\"Mean cosine(x, R_x(x))      : {rec_cos:.3f}\")\n",
    "        print(f\"Mean cosine(x, cyc(x))      : {cyc_cos:.3f}\")\n",
    "        print(f\"VSP corr (pairwise dots)    : {corr:.3f}\")\n",
    "        print(f\"Top-1 retrieved indices in Y: {top1[:min(5, len(top1))]} ... (k={k})\")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# CLI\n",
    "# -----------------------------\n",
    "def parse_args_tov():\n",
    "    p = argparse.ArgumentParser(description=\"Task-Operator vec2vec GAN (TO-v2vGAN)\")\n",
    "    # data\n",
    "    p.add_argument(\"--data_x\", type=str, default=\"\", help=\".npy file for X-space (descriptions)\")\n",
    "    p.add_argument(\"--data_y\", type=str, default=\"\", help=\".npy file for Y-space (models)\")\n",
    "    p.add_argument(\"--paired_x\", type=str, default=\"\", help=\"optional paired X .npy\")\n",
    "    p.add_argument(\"--paired_y\", type=str, default=\"\", help=\"optional paired Y .npy\")\n",
    "    p.add_argument(\"--dim\", type=int, default=128, help=\"embedding dimension (synthetic)\")\n",
    "    p.add_argument(\"--n_x\", type=int, default=20000)\n",
    "    p.add_argument(\"--n_y\", type=int, default=20000)\n",
    "    p.add_argument(\"--mixtures\", type=int, default=6)\n",
    "    p.add_argument(\"--std\", type=float, default=0.6)\n",
    "    # model\n",
    "    p.add_argument(\"--z\", type=int, default=128)\n",
    "    p.add_argument(\"--rank_o\", type=int, default=16, help=\"low-rank bottleneck for O(z)\")\n",
    "    p.add_argument(\"--disc_width\", type=int, default=512)\n",
    "    p.add_argument(\"--disc_depth\", type=int, default=3)\n",
    "    # train\n",
    "    p.add_argument(\"--epochs\", type=int, default=20)\n",
    "    p.add_argument(\"--batch_size\", type=int, default=128)\n",
    "    p.add_argument(\"--lr_g\", type=float, default=2e-4)\n",
    "    p.add_argument(\"--lr_d\", type=float, default=2e-4)\n",
    "    p.add_argument(\"--lambda_rec\", type=float, default=1.0)\n",
    "    p.add_argument(\"--lambda_cyc\", type=float, default=10.0)\n",
    "    p.add_argument(\"--lambda_vsp\", type=float, default=1.0)\n",
    "    p.add_argument(\"--lambda_ortho\", type=float, default=0.1)\n",
    "    p.add_argument(\"--lambda_energy\", type=float, default=0.01)\n",
    "    p.add_argument(\"--lambda_nce\", type=float, default=0.0)\n",
    "    p.add_argument(\"--lambda_l2\", type=float, default=0.0)\n",
    "    p.add_argument(\"--vsp_subset\", type=int, default=64)\n",
    "    p.add_argument(\"--log_interval\", type=int, default=200)\n",
    "    p.add_argument(\"--reortho_every\", type=int, default=1000, help=\"steps between P re-orthogonalization; 0=off\")\n",
    "    p.add_argument(\"--save_ckpt\", action=\"store_true\")\n",
    "    # misc\n",
    "    p.add_argument(\"--cpu\", action=\"store_true\")\n",
    "    p.add_argument(\"--seed\", type=int, default=42)\n",
    "    return p.parse_args()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    args = parse_args_tov()\n",
    "    if bool(args.data_x) ^ bool(args.data_y):\n",
    "        raise SystemExit(\"Provide both --data_x and --data_y, or neither (synthetic).\")\n",
    "    if bool(args.paired_x) ^ bool(args.paired_y):\n",
    "        raise SystemExit(\"Provide both --paired_x and --paired_y, or neither.\")\n",
    "    model, ds = train(args)\n",
    "    dev = next(model.parameters()).device\n",
    "    demo(model, ds, k=8, device=dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4043afbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Metamodel-Constrained Graph Decoder (Skeleton)\n",
    "----------------------------------------------\n",
    "Condition on a model embedding y (or \\hat{y} = F_{x->y}(x)) and decode a typed graph\n",
    "(e.g., UML/ArchiMate/OntoUML) that respects a given metamodel.\n",
    "\n",
    "This is a minimal, extensible scaffold illustrating:\n",
    "  • Metamodel schema (node/edge types, allowed incidence)\n",
    "  • Validity checks during autoregressive generation (masking invalid actions)\n",
    "  • Training with teacher forcing from gold graphs\n",
    "\n",
    "You should replace Dataset, featureization, and training loop to fit your data.\n",
    "\"\"\"\n",
    "\n",
    "# ---------------- Metamodel schema ----------------\n",
    "@dataclass\n",
    "class MetaModel:\n",
    "    node_types: List[str]\n",
    "    edge_types: List[str]\n",
    "    # adjacency constraints: (edge_type) -> (src_allowed_types, dst_allowed_types)\n",
    "    constraints: Dict[str, Tuple[List[str], List[str]]]\n",
    "\n",
    "    def node_type_idx(self) -> Dict[str, int]:\n",
    "        return {t:i for i,t in enumerate(self.node_types)}\n",
    "    def edge_type_idx(self) -> Dict[str, int]:\n",
    "        return {t:i for i,t in enumerate(self.edge_types)}\n",
    "\n",
    "# Example tiny ArchiMate-like subset\n",
    "ARCHI_TINY = MetaModel(\n",
    "    node_types=[\"BusinessActor\",\"BusinessRole\",\"ApplicationComponent\",\"DataObject\"],\n",
    "    edge_types=[\"Assignment\",\"Access\",\"Serving\"],\n",
    "    constraints={\n",
    "        \"Assignment\": ([\"BusinessActor\",\"BusinessRole\"],[\"BusinessRole\",\"ApplicationComponent\"]),\n",
    "        \"Access\": ([\"ApplicationComponent\"],[\"DataObject\"]),\n",
    "        \"Serving\": ([\"ApplicationComponent\"],[\"BusinessRole\",\"BusinessActor\"]),\n",
    "    }\n",
    ")\n",
    "\n",
    "# ---------------- Graph representation ----------------\n",
    "class TypedGraph:\n",
    "    def __init__(self, node_types: List[int], edges: List[Tuple[int,int,int]]):\n",
    "        \"\"\"edges: list of (src_idx, dst_idx, edge_type_idx)\"\"\"\n",
    "        self.node_types = node_types\n",
    "        self.edges = edges\n",
    "\n",
    "# ---------------- Decoder ----------------\n",
    "class GraphDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Autoregressive decoder with validity masking.\n",
    "    At each step it either (a) adds a node with a type, or (b) adds an edge between existing nodes with a valid edge type.\n",
    "    Conditioning: concatenates \\hat{y} to the state.\n",
    "    \"\"\"\n",
    "    def __init__(self, y_dim: int, hidden: int, num_node_types: int, num_edge_types: int):\n",
    "        super().__init__()\n",
    "        self.y_proj = nn.Linear(y_dim, hidden)\n",
    "        self.state = nn.GRU(input_size=hidden, hidden_size=hidden, batch_first=True)\n",
    "        self.node_head = nn.Linear(hidden, num_node_types)\n",
    "        self.edge_head = nn.Linear(hidden, num_edge_types)\n",
    "        # scoring heads for picking src/dst among existing nodes\n",
    "        self.src_head = nn.Linear(hidden, 1)\n",
    "        self.dst_head = nn.Linear(hidden, 1)\n",
    "\n",
    "    def forward(self, y_emb: torch.Tensor, steps: int, mm: MetaModel,\n",
    "                teacher: Optional[TypedGraph] = None, tau: float = 0.0) -> TypedGraph:\n",
    "        \"\"\"Greedy/teacher-forced decode. tau>0 enables Gumbel noise for exploration.\"\"\"\n",
    "        device = y_emb.device\n",
    "        h = torch.tanh(self.y_proj(y_emb)).unsqueeze(1)  # (B=1,1,H) for simplicity\n",
    "        hs, _ = self.state(h)  # one tick to initialize\n",
    "        hidden = hs[:, -1, :]  # (1,H)\n",
    "        node_types: List[int] = []\n",
    "        edges: List[Tuple[int,int,int]] = []\n",
    "\n",
    "        et2i = mm.edge_type_idx()\n",
    "\n",
    "        # add at least one node\n",
    "        logits_nt = self.node_head(hidden)  # (1, num_node_types)\n",
    "        probs_nt = F.softmax(logits_nt, dim=-1)\n",
    "        if tau > 0:\n",
    "            g = -torch.log(-torch.log(torch.rand_like(probs_nt)))\n",
    "            probs_nt = F.softmax((logits_nt + g) / max(1e-6, tau), dim=-1)\n",
    "        nt = int(torch.argmax(probs_nt, dim=-1))\n",
    "        node_types.append(nt)\n",
    "\n",
    "        # subsequent steps: alternately add node or edge\n",
    "        for t in range(steps):\n",
    "            # re-encode a simple state vector: counts and last type\n",
    "            state_vec = torch.cat([\n",
    "                hidden,\n",
    "                F.one_hot(torch.tensor([nt], device=device), num_classes=len(mm.node_types)).float()\n",
    "            ], dim=-1)\n",
    "            hs, _ = self.state(state_vec.unsqueeze(1), None)\n",
    "            hidden = hs[:, -1, :]\n",
    "\n",
    "            # decide to add node vs edge (heuristic: add edges after at least 2 nodes)\n",
    "            add_edge = (len(node_types) >= 2)\n",
    "            if not add_edge:\n",
    "                logits_nt = self.node_head(hidden)\n",
    "                nt = int(torch.argmax(F.softmax(logits_nt, dim=-1), dim=-1))\n",
    "                node_types.append(nt)\n",
    "                continue\n",
    "\n",
    "            # choose edge type with validity mask\n",
    "            logits_et = self.edge_head(hidden).squeeze(0)\n",
    "            mask = torch.zeros_like(logits_et)\n",
    "            # any pair of nodes whose types satisfy constraint is valid\n",
    "            valid_any = False\n",
    "            for e_name,(src_ok,dst_ok) in mm.constraints.items():\n",
    "                e_idx = et2i[e_name]\n",
    "                # check if at least one valid pair exists now\n",
    "                for s in range(len(node_types)):\n",
    "                    for d in range(len(node_types)):\n",
    "                        if s==d: \n",
    "                            continue\n",
    "                        if (mm.node_types[node_types[s]] in src_ok) and (mm.node_types[node_types[d]] in dst_ok):\n",
    "                            mask[e_idx] = 1.0\n",
    "                            valid_any = True\n",
    "                            break\n",
    "                    if valid_any:\n",
    "                        break\n",
    "            if not valid_any:\n",
    "                # fallback: add another node\n",
    "                logits_nt = self.node_head(hidden)\n",
    "                nt = int(torch.argmax(F.softmax(logits_nt, dim=-1), dim=-1))\n",
    "                node_types.append(nt)\n",
    "                continue\n",
    "            probs_et = F.softmax(logits_et.masked_fill(mask<0.5, -1e9), dim=-1)\n",
    "            et = int(torch.argmax(probs_et))\n",
    "\n",
    "            # select src/dst among nodes with masks\n",
    "            src_scores = torch.zeros(len(node_types), device=device)\n",
    "            dst_scores = torch.zeros(len(node_types), device=device)\n",
    "            for i in range(len(node_types)):\n",
    "                src_scores[i] = self.src_head(hidden).squeeze()\n",
    "                dst_scores[i] = self.dst_head(hidden).squeeze()\n",
    "            # pick the first valid pair greedily\n",
    "            chosen = None\n",
    "            e_name = mm.edge_types[et]\n",
    "            src_ok, dst_ok = mm.constraints[e_name]\n",
    "            for s in range(len(node_types)):\n",
    "                for d in range(len(node_types)):\n",
    "                    if s==d: \n",
    "                        continue\n",
    "                    if (mm.node_types[node_types[s]] in src_ok) and (mm.node_types[node_types[d]] in dst_ok):\n",
    "                        chosen = (s,d)\n",
    "                        break\n",
    "                if chosen: \n",
    "                    break\n",
    "            if chosen is not None:\n",
    "                s,d = chosen\n",
    "                edges.append((s,d,et))\n",
    "            else:\n",
    "                # no valid pair; skip\n",
    "                pass\n",
    "\n",
    "        return TypedGraph(node_types, edges)\n",
    "\n",
    "\n",
    "# ---------------- Training skeleton ----------------\n",
    "class GraphDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Placeholder: return (y_embedding, gold_graph) pairs.\"\"\"\n",
    "    def __init__(self, Y: torch.Tensor, graphs: List[TypedGraph]):\n",
    "        self.Y = Y\n",
    "        self.graphs = graphs\n",
    "    def __len__(self): return len(self.graphs)\n",
    "    def __getitem__(self, i): return self.Y[i], self.graphs[i]\n",
    "\n",
    "\n",
    "def teacher_force_loss(dec: GraphDecoder, y: torch.Tensor, gold: TypedGraph, mm: MetaModel):\n",
    "    \"\"\"Sketch: cross-entropy over node types and edge triplets with constraint masks.\"\"\"\n",
    "    device = y.device\n",
    "    # For brevity we mock a tiny supervision: encourage first node type\n",
    "    out_first = dec.node_head(torch.tanh(dec.y_proj(y))).squeeze(0)\n",
    "    target_nt0 = torch.tensor(gold.node_types[0], device=device)\n",
    "    return F.cross_entropy(out_first.unsqueeze(0), target_nt0.unsqueeze(0))\n",
    "\n",
    "\n",
    "def train_decoder(dec: GraphDecoder, ds: GraphDataset, mm: MetaModel, epochs=5, lr=1e-3):\n",
    "    opt = torch.optim.Adam(dec.parameters(), lr=lr)\n",
    "    for e in range(1, epochs+1):\n",
    "        total = 0.0\n",
    "        for y, g in ds:\n",
    "            y = y.unsqueeze(0)  # (1, D)\n",
    "            loss = teacher_force_loss(dec, y, g, mm)\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            total += float(loss.item())\n",
    "        print(f\"[Decoder] Epoch {e} loss={total/len(ds):.4f}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Tiny smoke test\n",
    "    D = 64\n",
    "    y = torch.randn(1, D)\n",
    "    dec = GraphDecoder(y_dim=D, hidden=128, num_node_types=len(ARCHI_TINY.node_types),\n",
    "                       num_edge_types=len(ARCHI_TINY.edge_types))\n",
    "    tg = dec(y, steps=5, mm=ARCHI_TINY)\n",
    "    print(\"Nodes:\", [ARCHI_TINY.node_types[t] for t in tg.node_types])\n",
    "    print(\"Edges:\", [(s,d,ARCHI_TINY.edge_types[e]) for (s,d,e) in tg.edges])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d9ac87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"datasets/eamodelset_nl2cm_embedding.pkl\", \"rb\") as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf8d588",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(978, 1536)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.stack(data['CM_Serialization_Emb']).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a644f4cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
